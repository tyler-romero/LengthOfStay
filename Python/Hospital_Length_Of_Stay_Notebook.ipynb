{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital Length of Stay\n",
    "\n",
    "In order for hospitals to optimize resource allocation, it is important to predict accurately how long a newly admitted patient will stay in the hospital.\n",
    "\n",
    "This notebook takes advantage of the power of SQL Server and RevoScalePy. The tables are all stored in a SQL Server, and most of the computations are done by loading chunks of data in-memory instead of the whole dataset.\n",
    "\n",
    "It does the following: \n",
    "\n",
    " * **Step 0: Packages and Compute Contexts**\n",
    " * **Step 1: Processing and Cleaning**\n",
    " * **Step 2: Feature Engineering**\n",
    " * **Step 3: Training and Evalutating a Random Forest, Boosted Trees, Fast Trees, and a Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Packages and Compute Contexts\n",
    "\n",
    "#### In this step, we set up the connection string to access a SQL Server Database and load the necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING.\n",
    "# We recommend not using Internet Explorer as it does not support plotting, and may crash your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "import os, sys\n",
    "from numpy import mean\n",
    "from math import sqrt\n",
    "from pandas import Series, DataFrame, to_numeric\n",
    "import pyodbc\n",
    "\n",
    "from revoscalepy import RxInSqlServer, RxLocalSeq, rx_set_compute_context, rx_import, rx_data_step, rx_summary\n",
    "from revoscalepy import rx_get_var_names, RxSqlServerData, RxTextData, rx_serialize_model, rx_import, rx_data_step\n",
    "from revoscalepy import rx_set_compute_context, rx_import, rx_data_step, rx_summary\n",
    "from revoscalepy import rx_dforest, rx_btrees, rx_predict, RxOdbcData, rx_get_var_info, RxSqlServerData, rx_get_var_names\n",
    "from revoscalepy import RxInSqlServer, RxLocalSeq, rx_set_compute_context, rx_write_object\n",
    "\n",
    "from microsoftml import rx_fast_trees, rx_neural_network, adadelta_optimizer\n",
    "from microsoftml import rx_predict as ml_predict\n",
    "\n",
    "from length_of_stay_utils import train_test_split, create_formula, write_rts_model, evaluate_model, get_num_rows, drop_view, alter_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Compute Contexts: user to input Server Name, database name, UID and Password. \n",
    "from SQLConnection import *\n",
    "\n",
    "# Choose a database name and create it. \n",
    "db = \"Hospital\"\n",
    "\n",
    "## Create database. \n",
    "pyodbc_cnxn = pyodbc.connect(connection_string)\n",
    "pyodbc_cursor = pyodbc_cnxn.cursor()\n",
    "pyodbc_cursor.execute(\"if not exists(SELECT * FROM sys.databases WHERE name = '{}') CREATE DATABASE {};\".format(db, db))\n",
    "pyodbc_cursor.close()\n",
    "pyodbc_cnxn.commit()\n",
    "pyodbc_cnxn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-Processing and Cleaning\n",
    "\n",
    "In this step, we: \n",
    "\n",
    "**1.** Upload the data set to SQL.\n",
    "\n",
    "**2.** Clean the merged data set: we replace NAs with the mode (categorical variables) or mean (continuous variables).\n",
    "\n",
    "**Input:**  Data Set LengthOfStay.csv\n",
    "\n",
    "**Output:** Cleaned raw data set LoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<revoscalepy.computecontext.RxLocalSeq.RxLocalSeq at 0x1b0fd6af518>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the compute context to Local. \n",
    "rx_set_compute_context(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 100000, Total Rows Processed: 100000, Total Chunk Time: 4.272 seconds \n",
      "\n",
      "Elapsed time to compute low/high values and/or factor levels: 4.477 secs.\n",
      " \n",
      "Total Rows written: 100000, Total time: 5.052\n",
      "Rows Read: 100000, Total Rows Processed: 100000, Total Chunk Time: 8.087 seconds \n",
      "Data exported to SQL\n"
     ]
    }
   ],
   "source": [
    "# Upload the data set to SQL.\n",
    "## Point to the input data set while specifying the classes.\n",
    "file_path = \"..\\\\Data\"\n",
    "LoS_text = RxTextData(file = os.path.join(file_path, \"LengthOfStay.csv\"), column_info=col_type_info)\n",
    "\n",
    "## Upload the table to SQL. \n",
    "LengthOfStay_sql = RxSqlServerData(table = \"LengthOfStay\", connection_string = connection_string)\n",
    "rx_data_step(input_data = LoS_text, output_file = LengthOfStay_sql, overwrite = True)\n",
    "\n",
    "print(\"Data exported to SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 50000, Total Rows Processed: 50000, Total Chunk Time: 2.337 seconds\n",
      "Rows Read: 50000, Total Rows Processed: 100000, Total Chunk Time: 2.653 seconds \n",
      "Computation time: 5.151 seconds.\n",
      "No missing values.\n",
      "You can move to step 2.\n"
     ]
    }
   ],
   "source": [
    "# Determine if LengthOfStay has missing values\n",
    "\n",
    "table = \"LengthOfStay\"\n",
    "\n",
    "# First, get the names and types of the variables to be treated.\n",
    "data_sql = RxSqlServerData(table = table, connection_string = connection_string, stringsAsFactors = True)\n",
    "colnames = rx_get_var_names(data_sql)\n",
    "\n",
    "# Then, get the names of the variables that actually have missing values. Assumption: no NA in eid, lengthofstay, or dates. \n",
    "var = [x for x in colnames if x not in [\"eid\", \"lengthofstay\", \"vdate\", \"discharged\"]]\n",
    "f = \"+\".join(var)\n",
    "summary = rx_summary(formula = f, data = data_sql, by_term = True).summary_data_frame\n",
    "\n",
    "var_with_NA = summary[summary[\"MissingObs\"] > 0]\n",
    "\n",
    "method = None\n",
    "if var_with_NA.empty:\n",
    "    print(\"No missing values.\")\n",
    "    print(\"You can move to step 2.\")\n",
    "    missing = False\n",
    "else:\n",
    "    print(\"Variables containing missing values are:\")\n",
    "    print(var_with_NA)\n",
    "    print(\"Apply one of the methods below to fill missing values.\")\n",
    "    missing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to clean\n",
      "Data cleaned\n"
     ]
    }
   ],
   "source": [
    "# If applicable, NULL is replaced with the mode (categorical variables: integer or character) or mean (continuous variables).\n",
    "\n",
    "if(missing == False):\n",
    "    print(\"Nothing to clean\")\n",
    "    LengthOfStay_cleaned_sql = RxSqlServerData(table = table, connection_string = connection_string)\n",
    "else:\n",
    "    print(\"Fill with mode and mean\")\n",
    "\n",
    "    # Get the variables types (categortical vs. continuous)\n",
    "    categ_names = []\n",
    "    contin_names = []\n",
    "    for index, row in var_with_NA.iterrows():\n",
    "        nameSeries = var_with_NA[\"Name\"]\n",
    "        name = nameSeries.to_string().split()[-1]\n",
    "        if col_info[name][\"type\"] == \"numeric\":\n",
    "            contin_names.append(name)\n",
    "        else:\n",
    "            categ_names.append(name)\n",
    "\n",
    "    # Function to replace missing values with the mode (categorical variables) or mean (continuous variables)\n",
    "    def fill_NA_mode_mean(dataset, context):\n",
    "        data = DataFrame(dataset)\n",
    "        for name in categ_names:\n",
    "            data.loc[data[name].isnull(),name] = data[name].mode().iloc[0]\n",
    "        for name in contin_names:\n",
    "            data.loc[data[name].isnull(), name] = data[name].mean()\n",
    "        return data\n",
    "\n",
    "    # Apply this function to LengthOfStay by wrapping it up in rxDataStep. Output is written to LoS0.\n",
    "    # We drop the LoS0 view in case the SQL Stored Procedure was executed in the same database before.\n",
    "    pyodbc_cnxn = pyodbc.connect(connection_string)\n",
    "    drop_view(\"LoS0\", connection_string)\n",
    "\n",
    "    LoS0_sql = RxSqlServerData(table = \"LoS0\", connection_string = connection_string)\n",
    "    rx_data_step(input_data = LengthOfStay_sql, output_file = LoS0_sql, overwrite = True, transform_function = fill_NA_mode_mean)\n",
    "   \n",
    "    LengthOfStay_cleaned_sql = RxSqlServerData(table = \"LoS0\", connectionString = connection_string)    \n",
    "    \n",
    "print(\"Data cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "**1.** Standardize the continuous variables (Z-score).\n",
    "\n",
    "**2.** Create the variable number_of_issues: the number of preidentified medical conditions.\n",
    "\n",
    "**Input:** Data set before feature engineering LengthOfStay.\n",
    "\n",
    "**Output:** Data set with new features LoS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 50000, Total Rows Processed: 50000, Total Chunk Time: 1.594 seconds\n",
      "Rows Read: 50000, Total Rows Processed: 100000, Total Chunk Time: 1.584 seconds \n",
      "Computation time: 3.338 seconds.\n",
      "Rows Read: 100000, Total Rows Processed: 100000Total Rows written: 100000, Total time: 5.37\n",
      ", Total Chunk Time: 10.949 seconds \n",
      "Feature Engineering Completed\n"
     ]
    }
   ],
   "source": [
    "# Get the mean and standard deviation of continuous variables.\n",
    "col_list = rx_get_var_names(LengthOfStay_cleaned_sql)\n",
    "f = \"+\".join(col_list)\n",
    "summary = rx_summary(formula = f, data = LengthOfStay_cleaned_sql, by_term = True).summary_data_frame\n",
    "\n",
    "names = [\"hematocrit\", \"neutrophils\", \"sodium\", \"glucose\", \"bloodureanitro\", \"creatinine\", \"bmi\", \"pulse\", \"respiration\"]\n",
    "statistics = summary[summary[\"Name\"].isin(names)]\n",
    "statistics = statistics[[\"Name\", \"Mean\", \"StdDev\"]]\n",
    "\n",
    "# standardization transform function\n",
    "def standardize(data, context):\n",
    "    for n, row in statistics.iterrows():\n",
    "        data[[row[\"Name\"]]] = (data[[row[\"Name\"]]] - row[\"Mean\"])/row[\"StdDev\"]\n",
    "    return data\n",
    "\n",
    "# number_of_issues transform function\n",
    "def calculate_number_of_issues(data, context):\n",
    "    data[\"number_of_issues\"] = to_numeric(data[\"hemo\"]) + to_numeric(data[\"dialysisrenalendstage\"]) + to_numeric(data[\"asthma\"])\\\n",
    "                               + to_numeric(data[\"irondef\"]) + to_numeric(data[\"pneum\"]) + to_numeric(data[\"substancedependence\"])\\\n",
    "                               + to_numeric(data[\"psychologicaldisordermajor\"]) + to_numeric(data[\"depress\"])\\\n",
    "                               + to_numeric(data[\"psychother\"]) + to_numeric(data[\"fibrosisandother\"]) + to_numeric(data[\"malnutrition\"])\n",
    "    return data\n",
    "\n",
    "# Combine transform functions into one overarching transform\n",
    "def transform(dataset, context):\n",
    "    data = DataFrame(dataset)\n",
    "    data = standardize(data, context)\n",
    "    data = calculate_number_of_issues(data, context)\n",
    "    return data\n",
    "\n",
    "# We drop the LoS view in case the SQL Stored Procedure was executed in the same database before.\n",
    "drop_view(\"LoS\", connection_string)\n",
    "\n",
    "# Standardize the cleaned table by wrapping it up in rxDataStep. Output is written to LoS_standard.\n",
    "table_name = \"LengthOfStay\" if missing is False else \"LoS0\"\n",
    "LengthOfStay_cleaned_sql = RxSqlServerData(sql_query = \"SELECT * FROM [Hospital].[dbo].[{}]\".format(table_name),\n",
    "                                           connection_string = connection_string)\n",
    "LoS_sql = RxSqlServerData(table = \"LoS\", connection_string = connection_string)\n",
    "rx_data_step(input_data = LengthOfStay_cleaned_sql, output_file = LoS_sql, overwrite = True, transform_function = transform)\n",
    "\n",
    "# Converting number_of_issues to character with a SQL query because as.character in rxDataStep is crashing.\n",
    "alter_column(\"LoS\", \"number_of_issues\", \"varchar(2)\", connection_string)\n",
    "alter_column(\"LoS\", \"lengthofstay\", \"float\", connection_string)\n",
    "\n",
    "print(\"Feature Engineering Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and Evaluating the Models\n",
    "\n",
    "In this step we:\n",
    "\n",
    "**1.** Split randomly the data set LoS into a training (LoS_Train) and a testing (LoS_Test) set.\n",
    " \n",
    "**2.** Train a Random Forest, Boosted Trees, Fast Trees, and Neural Network models on LoS_Train, and save them to SQL. \n",
    "\n",
    "**3.** Score the models on LoS_Test.\n",
    "\n",
    "**Input:** Data set LoS.\n",
    "\n",
    "**Output:** Random forest, Boosted Trees, Fast Trees, and Neural Network models saved to SQL and performance metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bloodureanitro': {'type': 'numeric'}, 'vdate': {'type': 'factor'}, 'glucose': {'type': 'numeric'}, 'irondef': {'type': 'factor', 'levels': ['0', '1']}, 'hematocrit': {'type': 'numeric'}, 'dialysisrenalendstage': {'type': 'factor', 'levels': ['0', '1']}, 'discharged': {'type': 'factor'}, 'neutrophils': {'type': 'numeric'}, 'bmi': {'type': 'numeric'}, 'hemo': {'type': 'factor', 'levels': ['0', '1']}, 'creatinine': {'type': 'numeric'}, 'number_of_issues': {'type': 'factor', 'levels': ['0', '2', '1', '3', '4', '5', '6', '7', '8', '9']}, 'malnutrition': {'type': 'factor', 'levels': ['0', '1']}, 'eid': {'type': 'integer'}, 'lengthofstay': {'type': 'numeric'}, 'psychologicaldisordermajor': {'type': 'factor', 'levels': ['0', '1']}, 'pneum': {'type': 'factor', 'levels': ['0', '1']}, 'secondarydiagnosisnonicd9': {'type': 'factor', 'levels': ['4', '1', '2', '3', '0', '7', '6', '10', '8', '5', '9']}, 'fibrosisandother': {'type': 'factor', 'levels': ['0', '1']}, 'rcount': {'type': 'factor', 'levels': ['0', '5+', '1', '4', '2', '3']}, 'asthma': {'type': 'factor', 'levels': ['0', '1']}, 'psychother': {'type': 'factor', 'levels': ['0', '1']}, 'gender': {'type': 'factor', 'levels': ['F', 'M']}, 'pulse': {'type': 'numeric'}, 'depress': {'type': 'factor', 'levels': ['0', '1']}, 'facid': {'type': 'factor', 'levels': ['B', 'A', 'E', 'D', 'C']}, 'sodium': {'type': 'numeric'}, 'respiration': {'type': 'numeric'}, 'substancedependence': {'type': 'factor', 'levels': ['0', '1']}}\n"
     ]
    }
   ],
   "source": [
    "# Point to the SQL table with the data set for modeling. Strings will be converted to factors.\n",
    "LoS = RxSqlServerData(table = \"LoS\", connection_string = connection_string, strings_as_factors = True)\n",
    "\n",
    "print(col_type_and_factor_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting completed\n"
     ]
    }
   ],
   "source": [
    "# Randomly split the data into a training set and a testing set, with a splitting % p.\n",
    "# p % goes to the training set, and the rest goes to the testing set. Default is 70%.\n",
    "\n",
    "p = 70\n",
    "\n",
    "## Create the Train_Id table containing Lead_Id of training set.\n",
    "train_test_split(\"eid\", \"LoS\", \"Train_Id\", p, connection_string)\n",
    "\n",
    "## Point to the training set. It will be created on the fly when training models.\n",
    "variables_all = rx_get_var_names(LoS)\n",
    "variables_to_remove = [\"eid\", \"vdate\", \"discharged\", \"facid\"]\n",
    "training_variables = [x for x in variables_all if x not in variables_to_remove]\n",
    "LoS_Train = RxSqlServerData(sql_query = \"SELECT eid, {} FROM LoS WHERE eid IN (SELECT eid from Train_Id)\".format(\n",
    "    ', '.join(training_variables)), connection_string = connection_string, column_info = col_type_and_factor_info\n",
    ")\n",
    "\n",
    "## Point to the testing set. It will be created on the fly when testing models.\n",
    "LoS_Test = RxSqlServerData(sql_query = \"SELECT eid, {} FROM LoS WHERE eid NOT IN (SELECT eid from Train_Id)\".format(\n",
    "    ', '.join(training_variables)), connection_string = connection_string, column_info = col_type_and_factor_info\n",
    ")\n",
    "\n",
    "print(\"Splitting completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula:  lengthofstay ~ rcount + gender + dialysisrenalendstage + asthma + irondef + pneum + substancedependence + psychologicaldisordermajor + depress + psychother + fibrosisandother + malnutrition + hemo + hematocrit + neutrophils + sodium + glucose + bloodureanitro + creatinine + bmi + pulse + respiration + secondarydiagnosisnonicd9 + number_of_issues\n"
     ]
    }
   ],
   "source": [
    "# Write the formula after removing variables not used in the modeling.\n",
    "formula = create_formula(\"lengthofstay\", variables_all, variables_to_remove)\n",
    "print(\"Formula: \", formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 1, Total Rows Processed: 1, Total Chunk Time: 0.006 seconds \n"
     ]
    }
   ],
   "source": [
    "num_rows = get_num_rows(\"Train_Id\", connection_string)\n",
    "\n",
    "# Define functions to tune rx_dforest and rx_btrees\n",
    "def tune_rx_dforest(formula, data, n_tree_list, cp_list, cc):\n",
    "    print(\"Tuning rx_dforest\")\n",
    "    best_error = sys.maxsize\n",
    "    best_model = None\n",
    "    for nt in n_tree_list:\n",
    "        for cp in cp_list:\n",
    "            model = rx_dforest(formula=formula,\n",
    "                               data=data,\n",
    "                               n_tree=nt,\n",
    "                               cp=cp,\n",
    "                               min_split=int(sqrt(num_rows)),\n",
    "                               max_num_bins=int(sqrt(num_rows)),\n",
    "                               seed=5,\n",
    "                               compute_context=cc)\n",
    "            error = model.oob_err['oob.err'][model.ntree - 1]\n",
    "            print(\"OOB Error: {} \\t n_tree: {} \\t cp: {}\".format(error, nt, cp))\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_model = model\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def tune_rx_btrees(formula, data, n_tree_list, lr_list, cp_list, cc):\n",
    "    print(\"Tuning rx_btrees\")\n",
    "    best_error = sys.maxsize\n",
    "    best_model = None\n",
    "    for nt in n_tree_list:\n",
    "        for lr in lr_list:\n",
    "            for cp in cp_list:\n",
    "                model = rx_btrees(formula=formula,\n",
    "                                  data=data,\n",
    "                                  n_tree=nt,\n",
    "                                  learning_rate=lr,\n",
    "                                  cp=cp,\n",
    "                                  loss_function=\"gaussian\",\n",
    "                                  min_split=int(sqrt(num_rows)),\n",
    "                                  max_num_bins=int(sqrt(num_rows)),\n",
    "                                  seed=9,\n",
    "                                  compute_context=cc)\n",
    "                error = model.oob_err['oob.err'][model.ntree - 1]\n",
    "                print(\"OOB Error: {} \\t n_tree: {} \\t learning_rate: {} \\t cp: {}\".format(error, nt, lr, cp))\n",
    "                if error < best_error:\n",
    "                    print(\"^^^ New best model!\")\n",
    "                    best_error = error\n",
    "                    best_model = model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning rx_dforest\n",
      "Number of observations not available for this data source. 'numObs' set to 1e6.\n",
      "\n",
      "For this data source, explicitly set 'max_num_bins' and 'min_split', otherwise innacurate models can result.\n",
      "Please see rx_dtree 'max_num_bins' and 'min_split' for details.\n",
      "OOB Error: 0.7229219675064087 \t n_tree: 40 \t cp: 5e-05\n",
      "Training Regression RF done\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest.\n",
    "forest_model = tune_rx_dforest(formula, LoS_Train, n_tree_list=[40], cp_list=[0.00005], cc=sql)\n",
    "\n",
    "print(\"Training Regression RF done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 1, Total Rows Processed: 1\n",
      "Total Rows written: 1, Total time: 0.011\n",
      ", Total Chunk Time: 0.050 seconds \n",
      "RF model uploaded to SQL\n"
     ]
    }
   ],
   "source": [
    "# Save the Random Forest in SQL. The compute context is set to local in order to export the model.\n",
    "write_rts_model(forest_model, \"RF\", connection_string)\n",
    "\n",
    "print(\"RF model uploaded to SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning rx_btrees\n",
      "Number of observations not available for this data source. 'numObs' set to 1e6.\n",
      "\n",
      "For this data source, explicitly set 'max_num_bins' and 'min_split', otherwise innacurate models can result.\n",
      "Please see rx_dtree 'max_num_bins' and 'min_split' for details.\n",
      "OOB Error: 0.7843199372291565 \t n_tree: 40 \t learning_rate: 0.3 \t cp: 5e-05\n",
      "^^^ New best model!\n"
     ]
    }
   ],
   "source": [
    "# Train the Boosted Trees model. This tunes on the basis of minimizing oob error.\n",
    "boosted_model = tune_rx_btrees(formula, LoS_Train, n_tree_list=[40], lr_list=[0.3], cp_list=[0.00005], cc=sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 1, Total Rows Processed: 1\n",
      "Total Rows written: 1, Total time: 0.001\n",
      ", Total Chunk Time: 0.033 seconds \n"
     ]
    }
   ],
   "source": [
    "# Save the Boosted Trees in SQL. The compute context is set to Local in order to export the model.\n",
    "rx_set_compute_context(local)\n",
    "\n",
    "write_rts_model(boosted_model, \"GBT\", connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'unbalanced_sets' ignored for method 'regression'\n",
      "Elapsed time: 00:00:00.8574579\n",
      "Training Fast Trees done\n"
     ]
    }
   ],
   "source": [
    "# Train the Fast Trees model.\n",
    "fast_model = rx_fast_trees(formula=formula,\n",
    "                          data=LoS_Train,\n",
    "                          method=\"regression\",\n",
    "                          num_trees=40,\n",
    "                          learning_rate=0.2,\n",
    "                          split_fraction=5/24,\n",
    "                          min_split=10,\n",
    "                          compute_context=sql)\n",
    "\n",
    "print(\"Training Fast Trees done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 1, Total Rows Processed: 1\n",
      "Total Rows written: 1, Total time: 0.002\n",
      ", Total Chunk Time: 0.033 seconds \n",
      "Fast Trees model uploaded to SQL\n"
     ]
    }
   ],
   "source": [
    "# Save the Fast Trees in SQL. The compute context is set to Local in order to export the model.\n",
    "write_rts_model(fast_model, \"FT\", connection_string)\n",
    "\n",
    "print(\"Fast Trees model uploaded to SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:00.1026195\n",
      "Training Neural Network done\n"
     ]
    }
   ],
   "source": [
    "# Train the Neural Network model.\n",
    "NN_model = rx_neural_network(formula=formula,\n",
    "                            data=LoS_Train,\n",
    "                            method=\"regression\",\n",
    "                            num_hidden_nodes=128,\n",
    "                            num_iterations=100,\n",
    "                            optimizer=adadelta_optimizer(),\n",
    "                            mini_batch_size=20,\n",
    "                            compute_context=sql)\n",
    "\n",
    "print(\"Training Neural Network done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 1, Total Rows Processed: 1\n",
      "Total Rows written: 1, Total time: 0.001\n",
      ", Total Chunk Time: 0.034 seconds \n"
     ]
    }
   ],
   "source": [
    "write_rts_model(NN_model, \"NN\", connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 29938, Total Rows Processed: 29938, Total Chunk Time: 1.321 seconds\n",
      "Total Rows written: 29938, Total time: 0.499\n",
      " \n",
      "Rows Read: 29938, Total Rows Processed: 29938, Total Chunk Time: 0.031 seconds \n",
      "OrderedDict([('model_name', ['RF']), ('mean_absolute_error', [0.6188585992512238]), ('root_mean_squared_error', [0.84188309193952404]), ('relative_absolute_error', [0.32274628245733628]), ('relative_squared_error', [0.12710825557031963]), ('coefficient_of_determination', [0.87289174442968043])])\n",
      "Scoring Random Forest (rxDForest) done\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Scoring \n",
    "\n",
    "# Make Predictions, then import them into Python.\n",
    "forest_prediction_sql = RxSqlServerData(table = \"Forest_Prediction\",\n",
    "                                        strings_as_factors = True,\n",
    "                                        connection_string = connection_string)\n",
    "rx_predict(forest_model,\n",
    "           data = LoS_Test,\n",
    "           output_data = forest_prediction_sql,\n",
    "           type = \"response\",\n",
    "           extra_vars_to_write = [\"lengthofstay\", \"eid\"],\n",
    "           overwrite = True)\n",
    "\n",
    "# Compute the performance metrics of the model.\n",
    "forest_prediction = rx_import(input_data = forest_prediction_sql)\n",
    "forest_metrics = evaluate_model(observed = forest_prediction['lengthofstay'],\n",
    "                                predicted = forest_prediction['lengthofstay_Pred'],\n",
    "                                model = \"RF\")\n",
    "\n",
    "print(\"Scoring Random Forest (rxDForest) done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 29938, Total Rows Processed: 29938, Total Chunk Time: 1.286 seconds\n",
      "Total Rows written: 29938, Total time: 0.507\n",
      " \n",
      "Rows Read: 29938, Total Rows Processed: 29938, Total Chunk Time: 0.031 seconds \n",
      "OrderedDict([('model_name', ['GBT']), ('mean_absolute_error', [0.6511701891108345]), ('root_mean_squared_error', [0.88104416506831673]), ('relative_absolute_error', [0.33959737820052077]), ('relative_squared_error', [0.13920843219290197]), ('coefficient_of_determination', [0.86079156780709809])])\n",
      "Scoring Boosted Trees (rx_btrees) done\n"
     ]
    }
   ],
   "source": [
    "# Boosted Trees Scoring\n",
    "## Make Predictions, then import them into R. \n",
    "boosted_prediction_sql = RxSqlServerData(table = \"Boosted_Prediction\",\n",
    "                                         strings_as_factors = True,\n",
    "                                         connection_string = connection_string)\n",
    "rx_predict(boosted_model,\n",
    "           data = LoS_Test,\n",
    "           output_data = boosted_prediction_sql,\n",
    "           extra_vars_to_write = [\"lengthofstay\", \"eid\"],\n",
    "           overwrite = True)\n",
    "\n",
    "# Compute the performance metrics of the model.\n",
    "boosted_prediction = rx_import(input_data = boosted_prediction_sql)\n",
    "boosted_metrics = evaluate_model(observed = boosted_prediction['lengthofstay'],\n",
    "                                 predicted = boosted_prediction['lengthofstay_Pred'],\n",
    "                                 model = \"GBT\")\n",
    "\n",
    "print(\"Scoring Boosted Trees (rx_btrees) done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Read: 29938, Total Rows Processed: 29938, Total Chunk Time: 1.205 seconds \n",
      "Beginning processing data.\n",
      "Rows Read: 29938, Read Time: 0, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Elapsed time: 00:00:00.5841457\n",
      "Finished writing 29938 rows.\n",
      "Writing completed.\n",
      "Rows Read: 29938, Total Rows Processed: 29938\n",
      "Total Rows written: 29938, Total time: 0.491\n",
      ", Total Chunk Time: 0.536 seconds \n",
      "OrderedDict([('model_name', ['FT']), ('mean_absolute_error', [0.3701010029037536]), ('root_mean_squared_error', [0.52396516787604697]), ('relative_absolute_error', [0.19301456417579552]), ('relative_squared_error', [0.049235121909362738]), ('coefficient_of_determination', [0.95076487809063726])])\n",
      "Scoring Fast Trees (rx_fast_trees) done\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions, then write them to a table.\n",
    "LoS_Test_import = rx_import(input_data=LoS_Test)\n",
    "fast_prediction = ml_predict(fast_model, data=LoS_Test_import, extra_vars_to_write=[\"lengthofstay\", \"eid\"], overwrite=True)\n",
    "fast_prediction_sql = RxSqlServerData(table=\"Fast_Prediction\", strings_as_factors=True, connection_string=connection_string)\n",
    "rx_data_step(input_data=fast_prediction, output_file=fast_prediction_sql, overwrite=True)\n",
    "\n",
    "# Compute the performance metrics of the model.\n",
    "fast_metrics = evaluate_model(observed=fast_prediction['lengthofstay'], predicted=fast_prediction['Score'], model=\"FT\")\n",
    "\n",
    "print(\"Scoring Fast Trees (rx_fast_trees) done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning processing data.\n",
      "Rows Read: 29938, Read Time: 0.001, Transform Time: 0\n",
      "Beginning processing data.\n",
      "Elapsed time: 00:00:00.3213019\n",
      "Finished writing 29938 rows.\n",
      "Writing completed.\n",
      "Rows Read: 29938, Total Rows Processed: 29938\n",
      "Total Rows written: 29938, Total time: 0.486\n",
      ", Total Chunk Time: 0.602 seconds \n",
      "OrderedDict([('model_name', ['NN']), ('mean_absolute_error', [0.4887321793616547]), ('root_mean_squared_error', [0.70966139753137347]), ('relative_absolute_error', [0.25488293157289299]), ('relative_squared_error', [0.090317633153016302]), ('coefficient_of_determination', [0.90968236684698367])])\n",
      "Scoring Neural Networks (rx_neural_networks) done\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions, then write them to a table.\n",
    "NN_prediction = ml_predict(NN_model, data=LoS_Test_import, extra_vars_to_write=[\"lengthofstay\", \"eid\"], overwrite=True)\n",
    "NN_prediction_sql = RxSqlServerData(table=\"NN_Prediction\", strings_as_factors=True, connection_string=connection_string)\n",
    "rx_data_step(input_data=NN_prediction, output_file=NN_prediction_sql, overwrite=True)\n",
    "\n",
    "# Compute the performance metrics of the model.\n",
    "NN_metrics = evaluate_model(observed=NN_prediction['lengthofstay'], predicted=NN_prediction['Score'], model=\"NN\")\n",
    "\n",
    "print(\"Scoring Neural Networks (rx_neural_networks) done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
